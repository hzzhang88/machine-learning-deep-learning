{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dbbf61d-8eb3-4d87-b1ea-218fd0985024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7c09ffcb-bdcd-4a79-93d2-65e215aa011d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_on_feature(data,feature,threshold):\n",
    "    \"\"\"\n",
    "    Based on the threshold of the specified feaeture, the data (X,y) is split into two branches\n",
    "    feature: int\n",
    "    threshold:int\n",
    "    data: Numpy.array\n",
    "        The data inlcudes the X and y\n",
    "    \"\"\"\n",
    "    left_branch = None\n",
    "    right_branch = None\n",
    "    idx = (data[:,feature] >= threshold)\n",
    "    left_branch = data[idx]\n",
    "    right_branch = data[~idx]\n",
    "    return left_branch,right_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31cacbb9-0037-43b1-8642-7320eb5e28f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionNode():\n",
    "    \"\"\"Class that represents a decision node or leaf in the decision tree\n",
    "    Parameters:\n",
    "    -----------\n",
    "    feature_i: int\n",
    "        Feature index which we want to use as the threshold measure.\n",
    "    threshold: float\n",
    "        The value that we will compare feature values at feature_i against to\n",
    "        determine the prediction.\n",
    "    value: float\n",
    "        The class prediction if classification tree, or float value if regression tree.\n",
    "    true_branch: DecisionNode\n",
    "        Next decision node for samples where features value met the threshold.\n",
    "    false_branch: DecisionNode\n",
    "        Next decision node for samples where features value did not meet the threshold.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_i=None, threshold=None,\n",
    "                 value=None, true_branch=None, false_branch=None):\n",
    "        self.feature_i = feature_i          # Index for the feature that is tested\n",
    "        self.threshold = threshold          # Threshold value for feature\n",
    "        self.value = value                  # Value if the node is a leaf in the tree\n",
    "        self.true_branch = true_branch      # 'Left' subtree\n",
    "        self.false_branch = false_branch    # 'Right' subtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "561fda08-4077-44b4-9d23-45a448b95b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionTree(DecisionTree):\n",
    "    def _calculate_variance_reduction(self,y,y1,y2):\n",
    "        var_tot = calculate_variance(y)\n",
    "        var_1 = calculate_variance(y1)\n",
    "        var_2 = calculate_variance(y2)\n",
    "        frac_1 = len(y1)/len(y)\n",
    "        frac_2 = len(y2)/len(y)\n",
    "        \n",
    "        variance_reduction = var_tot - (frac_1*var_1 + frac_2*var_2)\n",
    "        \n",
    "        return sum(variance_reduction)\n",
    "    \n",
    "    def _mean_of_y(self,y):\n",
    "        value = np.mean(y,axis = 0)\n",
    "        return value if len(value) >1 else value[0]\n",
    "    \n",
    "    def fit(self, X,y):\n",
    "        self._impurity_calculation = self._calculate_variance_reduction\n",
    "        self._leaf_value_calculation = self._mean_of_y\n",
    "        super(RegressionTree,self).fit(X,y)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f437679-537a-4549-8698-6a25c80f81d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationTree(DecisionTree):\n",
    "    def _calculate_information_gain(self,y,y1,y2):\n",
    "        p = len(y1) / len(y)\n",
    "        entropy = calculate_entropy(y)\n",
    "        info_gain = entropy - p*calculate_entropy(y1) - (1-p)*calculate_entropy(y2)\n",
    "        \n",
    "        return info_gain\n",
    "    \n",
    "    def _majority_vote(self, y):\n",
    "        most_common = None\n",
    "        max_count = 0\n",
    "        for label in np.unique(y):\n",
    "            # Count number of occurences of samples with label\n",
    "            count = len(y[y == label])\n",
    "            if count > max_count:\n",
    "                most_common = label\n",
    "                max_count = count\n",
    "        return most_common\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self._impurity_calculation = self._calculate_information_gain\n",
    "        self._leaf_value_calculation = self._majority_vote\n",
    "        super(ClassificationTree, self).fit(X, y)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "35d7a045-21ac-40b8-a421-cd01f53d80dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gini(y):\n",
    "    y_labels = np.unique(y)\n",
    "    y_counts = len(y)\n",
    "    y_p ={}\n",
    "    gini = 1.\n",
    "    for y_label in y_labels:\n",
    "        y_p[y_label] = len(y[y == y_label])/y_counts\n",
    "        gini -=y_p[y_label]**2\n",
    "    return gini\n",
    "\n",
    "class CARTClassificationTree(DecisionTree):\n",
    "    def _calculate_gini_gain(self,y,y1,y2):\n",
    "        p = len(y1)/len(y)\n",
    "        gini = Gini(y)\n",
    "        gain = gini - p*Gini(y1)-(1-p)*Gini(y2)\n",
    "        return gain\n",
    "    \n",
    "    def _majority_vote(self, y):\n",
    "        most_common = None\n",
    "        max_count = 0\n",
    "        for label in np.unique(y):\n",
    "            # Count number of occurences of samples with label\n",
    "            count = len(y[y == label])\n",
    "            if count > max_count:\n",
    "                most_common = label\n",
    "                max_count = count\n",
    "        return most_common\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self._impurity_calculation = self._calculate_gini_gain\n",
    "        self._leaf_value_calculation = self._majority_vote\n",
    "        super(CARTClassificationTree, self).fit(X, y)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8ec5a00d-3581-4f8c-93f4-3a0305d23b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree(object):\n",
    "    \"\"\"Super class of RegressionTree and ClassificationTree.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    min_samples_split: int\n",
    "        The minimum number of samples needed to make a split when building a tree.\n",
    "    min_impurity: float\n",
    "        The minimum impurity required to split the tree further.\n",
    "    max_depth: int\n",
    "        The maximum depth of a tree.\n",
    "    loss: function\n",
    "        Loss function that is used for Gradient Boosting models to calculate impurity.\n",
    "    \"\"\"\n",
    "    def __init__(self, min_samples_split=2, min_impurity=1e-7,\n",
    "                 max_depth=float(\"inf\"), loss=None):\n",
    "        self.root = None  # Root node in dec. tree\n",
    "        # Minimum n of samples to justify split\n",
    "        self.min_samples_split = min_samples_split\n",
    "        # The minimum impurity to justify split\n",
    "        self.min_impurity = min_impurity\n",
    "        # The maximum depth to grow the tree to\n",
    "        self.max_depth = max_depth\n",
    "        # Function to calculate impurity (classif.=>info gain, regr=>variance reduct.)\n",
    "        self._impurity_calculation = None\n",
    "        # Function to determine prediction of y at leaf\n",
    "        self._leaf_value_calculation = None\n",
    "        # If y is one-hot encoded (multi-dim) or not (one-dim)\n",
    "        self.one_dim = None\n",
    "        # If Gradient Boost\n",
    "        self.loss = loss\n",
    "\n",
    "    def fit(self, X, y, loss=None):\n",
    "        \"\"\" Build decision tree \"\"\"\n",
    "        self.one_dim = len(np.shape(y)) == 1\n",
    "        self.root = self._build_tree(X, y)\n",
    "        self.loss=None\n",
    "\n",
    "    def _build_tree(self, X, y, current_depth=0):\n",
    "        \"\"\" Recursive method which builds out the decision tree and splits X and respective y\n",
    "        on the feature of X which (based on impurity) best separates the data\"\"\"\n",
    "\n",
    "        largest_impurity = 0\n",
    "        best_criteria = None    # Feature index and threshold\n",
    "        best_sets = None        # Subsets of the data\n",
    "\n",
    "        # Check if expansion of y is needed\n",
    "        if len(np.shape(y)) == 1:\n",
    "            y = np.expand_dims(y, axis=1)\n",
    "\n",
    "        # Add y as last column of X\n",
    "        Xy = np.concatenate((X, y), axis=1)\n",
    "\n",
    "        n_samples, n_features = np.shape(X)\n",
    "\n",
    "        if n_samples >= self.min_samples_split and current_depth <= self.max_depth:\n",
    "            # Calculate the impurity for each feature\n",
    "            for feature_i in range(n_features):\n",
    "                # All values of feature_i\n",
    "                feature_values = np.expand_dims(X[:, feature_i], axis=1)\n",
    "                unique_values = np.unique(feature_values)\n",
    "\n",
    "                # Iterate through all unique values of feature column i and\n",
    "                # calculate the impurity\n",
    "                for threshold in unique_values:\n",
    "                    # Divide X and y depending on if the feature value of X at index feature_i\n",
    "                    # meets the threshold\n",
    "                    Xy1, Xy2 = divide_on_feature(Xy, feature_i, threshold)\n",
    "\n",
    "                    if len(Xy1) > 0 and len(Xy2) > 0:\n",
    "                        # Select the y-values of the two sets\n",
    "                        y1 = Xy1[:, n_features:]\n",
    "                        y2 = Xy2[:, n_features:]\n",
    "\n",
    "                        # Calculate impurity\n",
    "                        impurity = self._impurity_calculation(y, y1, y2)\n",
    "\n",
    "                        # If this threshold resulted in a higher information gain than previously\n",
    "                        # recorded save the threshold value and the feature\n",
    "                        # index\n",
    "                        if impurity > largest_impurity:\n",
    "                            largest_impurity = impurity\n",
    "                            best_criteria = {\"feature_i\": feature_i, \"threshold\": threshold}\n",
    "                            best_sets = {\n",
    "                                \"leftX\": Xy1[:, :n_features],   # X of left subtree\n",
    "                                \"lefty\": Xy1[:, n_features:],   # y of left subtree\n",
    "                                \"rightX\": Xy2[:, :n_features],  # X of right subtree\n",
    "                                \"righty\": Xy2[:, n_features:]   # y of right subtree\n",
    "                                }\n",
    "\n",
    "        if largest_impurity > self.min_impurity:\n",
    "            # Build subtrees for the right and left branches\n",
    "            true_branch = self._build_tree(best_sets[\"leftX\"], best_sets[\"lefty\"], current_depth + 1)\n",
    "            false_branch = self._build_tree(best_sets[\"rightX\"], best_sets[\"righty\"], current_depth + 1)\n",
    "            return DecisionNode(feature_i=best_criteria[\"feature_i\"], threshold=best_criteria[\n",
    "                                \"threshold\"], true_branch=true_branch, false_branch=false_branch)\n",
    "\n",
    "        # We're at leaf => determine value\n",
    "        leaf_value = self._leaf_value_calculation(y)\n",
    "\n",
    "        return DecisionNode(value=leaf_value)\n",
    "\n",
    "\n",
    "    def predict_value(self, x, tree=None):\n",
    "        \"\"\" Do a recursive search down the tree and make a prediction of the data sample by the\n",
    "            value of the leaf that we end up at \"\"\"\n",
    "\n",
    "        if tree is None:\n",
    "            tree = self.root\n",
    "\n",
    "        # If we have a value (i.e we're at ja leaf) => return value as the prediction\n",
    "        if tree.value is not None:\n",
    "            return tree.value\n",
    "\n",
    "        # Choose the feature that we will test\n",
    "        feature_value = x[tree.feature_i]\n",
    "\n",
    "        # Determine if we will follow left or right branch\n",
    "        branch = tree.false_branch\n",
    "        if isinstance(feature_value, int) or isinstance(feature_value, float):\n",
    "            if feature_value >= tree.threshold:\n",
    "                branch = tree.true_branch\n",
    "        elif feature_value == tree.threshold:\n",
    "            branch = tree.true_branch\n",
    "\n",
    "        # Test subtree\n",
    "        return self.predict_value(x, branch)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Classify samples one by one and return the set of labels \"\"\"\n",
    "        y_pred = [self.predict_value(sample) for sample in X]\n",
    "        return y_pred\n",
    "\n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        \"\"\" Recursively print the decision tree \"\"\"\n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "\n",
    "        # If we're at leaf => print the label\n",
    "        if tree.value is not None:\n",
    "            print (tree.value)\n",
    "        # Go deeper down the tree\n",
    "        else:\n",
    "            # Print test\n",
    "            print (\"%s:%s? \" % (tree.feature_i, tree.threshold))\n",
    "            # Print the true scenario\n",
    "            print (\"%sT->\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.true_branch, indent + indent)\n",
    "            # Print the false scenario\n",
    "            print (\"%sF->\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.false_branch, indent + indent)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2cdc5ed1-3326-445a-9e18-d30f59f69c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "K =20\n",
    "p =2\n",
    "N =300\n",
    "mu = np.random.normal(0.,4.,(K,2))\n",
    "component = np.random.randint(0,K,(N,))\n",
    "assignment = np.random.randint(0,2,(K,))\n",
    "X = mu[component,:] + np.random.normal(0.,1.,(N,p))\n",
    "y = assignment[component]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c246e969-a5d4-4d19-bcb8-e161282c1a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7833333333333333"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = CARTClassificationTree(max_depth =2)\n",
    "f.fit(X,y)\n",
    "pred_values = f.predict(X)\n",
    "(pred_values == y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6d89db89-d4ed-4748-8d67-6cb866e5c702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:-1.6572172525889342? \n",
      " T->1:-8.828687747795405? \n",
      "  T->1.0\n",
      "  F->0:-2.37762472288401? \n",
      "    T->1.0\n",
      "    F->0.0\n",
      " F->0:-4.457559665582504? \n",
      "  T->1:2.4972787399988756? \n",
      "    T->1.0\n",
      "    F->0.0\n",
      "  F->0:-2.919629584914988? \n",
      "    T->0.0\n",
      "    F->0.0\n"
     ]
    }
   ],
   "source": [
    "f.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39742692-0570-4460-b2d9-982a57497238",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
