{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "976d96ea-ed3a-425b-b401-541ccd4aa20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class DecisionNode():\n",
    "    def __init__(self,feature_i = None, threshold = None, value = None,\n",
    "                 true_branch = None, false_branch = None):\n",
    "        self.feature_i = feature_i \n",
    "        self.threshold = threshold\n",
    "        self.value = value\n",
    "        self.true_branch = true_branch\n",
    "        self.false_branch = false_branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d909a538-100b-48bc-bf0b-4176f1a66546",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XgboostTree():\n",
    "    \n",
    "    def __init__(self, min_samples_split =2, min_score  = 1e-7,\n",
    "                max_depth = float(\"inf\"), loss = None, lambda_ = 1, gamma_ = 1,eps = 0.1 ):\n",
    "        self.root = None\n",
    "        self.max_depth = max_depth\n",
    "        self.lambda_ = lambda_\n",
    "        self.gamma_ = gamma_\n",
    "        self.eps = eps\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_score= min_score\n",
    "        self.loss = loss\n",
    "    \n",
    "    def split_on_feature(self,X,y,g,h,feature_i,value):\n",
    "        idx = X[:,feature_i]<=value\n",
    "        \n",
    "        Xl,Xr = X[idx],X[~idx]\n",
    "        yl,yr = y[idx],y[~idx]\n",
    "        gl,gr = g[idx],g[~idx]\n",
    "        hl,hr = h[idx],h[~idx]\n",
    "        return Xl,yl,gl,hl,Xr,yr,gr,hr\n",
    "    \n",
    "    \n",
    "    def find_best_split(self,X,y,g,h):\n",
    "        score = np.NINF\n",
    "        split_col_value = None\n",
    "        G = g.sum()\n",
    "        H = h.sum()\n",
    "        n_samples, n_features = np.shape(X)\n",
    "        for feature_i in range(n_features):\n",
    "            G_L = 0.; H_L = 0.\n",
    "            for j in np.argsort(X[:,feature_i]):\n",
    "                G_L += g[j]\n",
    "                H_L += h[j]\n",
    "                G_R = G - G_L\n",
    "                H_R = H-H_L\n",
    "                \n",
    "                score_new = (G_L**2)/(H_L +self.lambda_) +(G_R**2)/(H_R +self.lambda_) -(G**2)/(H +self.lambda_) - self.gamma_\n",
    "                \n",
    "                if score_new > score:\n",
    "                    score = score_new\n",
    "                    split_col_value = {'score':score,'feature_i':feature_i,'split_value':X[j,feature_i] }\n",
    "        return split_col_value\n",
    "    \n",
    "    def calc_leaf(self,g,h):\n",
    "        w = -g.sum()/(h.sum()+self.lambda_)\n",
    "        return w \n",
    "    \n",
    "    def create_tree(self,X,y,g,h,current_depth = 0):\n",
    "        \n",
    "        n_samples, n_features = np.shape(X)\n",
    "        if n_samples >= self.min_samples_split and current_depth <= self.max_depth:\n",
    "            \n",
    "            if self.eps is None:\n",
    "                split_col_value = self.find_best_split(X,y,g,h)\n",
    "            else:\n",
    "                split_col_value = self.approximate_split(X,y,g,h)\n",
    "                \n",
    "            if split_col_value is not None and split_col_value['score'] > self.min_score:\n",
    "                Xl,yl,gl,hl,Xr,yr,gr,hr = self.split_on_feature(X,y,g,h,\n",
    "                                                       split_col_value['feature_i'],\n",
    "                                                       split_col_value['split_value'])\n",
    "                true_branch = self.create_tree(Xl,yl,gl,hl,current_depth +1)\n",
    "                false_branch = self.create_tree(Xr,yr,gr,hr, current_depth+1)  \n",
    "                return DecisionNode(feature_i =split_col_value['feature_i'],threshold =split_col_value['split_value'],\n",
    "                               true_branch = true_branch, false_branch = false_branch)  \n",
    "        leaf_value = self.calc_leaf(g,h)\n",
    "        return DecisionNode(value = leaf_value)\n",
    "    \n",
    "    \n",
    "    def predict_value(self,x,tree = None):\n",
    "        if tree is None:\n",
    "            tree = self.root\n",
    "            \n",
    "        if tree.value is not None:\n",
    "            return tree.value\n",
    "        \n",
    "        feature_values = x[tree.feature_i]\n",
    "        branch = tree.false_branch\n",
    "        if isinstance(feature_values, int) or isinstance(feature_values, float):\n",
    "            if feature_values <= tree.threshold:\n",
    "                branch = tree.true_branch\n",
    "            elif feature_values == tree.threshold:\n",
    "                branch = tree.true_branch\n",
    "        \n",
    "        return self.predict_value(x,branch)\n",
    "    \n",
    "    def predict(self,X):\n",
    "        y_pred = [self.predict_value(sample) for sample in X]\n",
    "        return np.array(y_pred)\n",
    "    \n",
    "    \n",
    "    def calc_grad_hessain(self,y,pred_y):\n",
    "        _,g,h = self.loss(y,pred_y)\n",
    "        return g,h\n",
    "    \n",
    "    def fit(self,X,y,pred_y):\n",
    "        g,h = self.calc_grad_hessain(y,pred_y)\n",
    "        self.root = self.create_tree(X,y,g,h)\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def approximate_split(self,X,y,g,h):\n",
    "        \"\"\"\n",
    "        Approximate algorithm using the weigted quantile sketch\n",
    "        \"\"\"\n",
    "        score = np.NINF\n",
    "        split_col_value = None\n",
    "        G = g.sum()\n",
    "        H = h.sum()\n",
    "        n_samples, n_features = np.shape(X)\n",
    "        for feature_i in range(n_features):\n",
    "            x_h = np.vstack([X[:,feature_i],g,h]).T\n",
    "            x_h = x_h[x_h[:,0].argsort()]\n",
    "            x_h[:,2] = x_h[:,2]/x_h[:,2].sum()\n",
    "            hh = x_h[:,2].cumsum()\n",
    "            start = 0;j=1\n",
    "            while True:\n",
    "                if hh[j]-start >= self.eps:\n",
    "                    start = hh[j-1]\n",
    "                    G_L = x_h[:j,1].sum(); H_L =x_h[:j,2].sum()\n",
    "                    G_R = G-G_L; H_R = H-H_L\n",
    "                    score_new = (G_L**2)/(H_L +self.lambda_) +(G_R**2)/(H_R +self.lambda_) -(G**2)/(H +self.lambda_) - self.gamma_\n",
    "                    if score_new > score:\n",
    "                        score = score_new\n",
    "                        split_col_value = {'score':score,'feature_i':feature_i,'split_value':x_h[j-1,0] }\n",
    "                        \n",
    "                    # G_L = x_h[:(j+1),1].sum(); H_L =x_h[:(j+1),2].sum() \n",
    "                    # G_R = G-G_L; H_R = H-H_L\n",
    "                    # score_new = (G_L**2)/(H_L +self.lambda_) +(G_R**2)/(H_R +self.lambda_) -(G**2)/(H +self.lambda_)\n",
    "                    # if score_new > score:\n",
    "                    #     score = score_new\n",
    "                    #     split_col_value = {'score':score,'feature_i':feature_i,'split_value':X[j,feature_i] }\n",
    "                j +=1\n",
    "                if j == n_samples:\n",
    "                    break   \n",
    "        return split_col_value\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cc0fcbd0-59a1-4ac4-b93a-819c713ed307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_loss(y,pred_y):\n",
    "    loss = np.sum((y-pred_y)**2)\n",
    "    g = 2*(pred_y - y)\n",
    "    h = np.full(g.shape,2.)\n",
    "    return loss,g,h\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "# first order gradient logLoss\n",
    "def sigmoid_loss( labels,preds):\n",
    "    preds = sigmoid(preds)\n",
    "    loss =-(labels*np.log(preds) + (1-labels)*np.log(1-preds)).mean()\n",
    "    g =preds - labels \n",
    "    h =preds * (1 - preds)\n",
    "    return loss,g,h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f558edb9-db20-4d8f-b558-12ad9119eb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class xgboost:\n",
    "    \"\"\"\n",
    "    n_iters: the number of weak learners.\n",
    "    lr: the learning rate of each learner.\n",
    "    \"\"\"\n",
    "    def __init__(self,n_iters = 10,lr = 0.1,loss = None ):\n",
    "        self.n_iters = 10\n",
    "        self.loss = loss\n",
    "        self.lr = lr\n",
    "        self.models = []\n",
    "    def fit(self,X,y,min_samples_split =2, min_score  = 1e-7,\n",
    "                max_depth = float(\"inf\"), lambda_ = 1, gamma_ = 1,eps = 0.1):\n",
    "        y0 = np.full(y.shape,1.)\n",
    "        y_pred = y0.flatten()\n",
    "        for epoch in range(self.n_iters):\n",
    "            boosting_tree = XgboostRegressionTree(loss =self.loss,eps = eps,min_samples_split =min_samples_split, min_score  = min_score,\n",
    "                max_depth = max_depth, lambda_ = lambda_, gamma_ = gamma_)\n",
    "            boosting_tree.fit(X,y,y_pred)\n",
    "            y_pred += self.lr*boosting_tree.predict(X)\n",
    "            self.models.append(boosting_tree)\n",
    "    def predict(self,X):\n",
    "        pred = np.zeros(X.shape[0])\n",
    "        for model in self.models:\n",
    "            pred += self.lr*model.predict(X)\n",
    "        return np.full((X.shape[0], 1), 1.).flatten().astype('float64') + pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5bfd98b9-1641-4914-af68-382801d0c2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost(X,y,n_iters =5,loss =square_loss,eps = 0.1):\n",
    "    models = []\n",
    "    y0 = np.full(y.shape,1.)\n",
    "    y_pred = y0.flatten()\n",
    "    for epoch in range(n_iters):\n",
    "        boosting_tree = XgboostTree(loss =loss,eps = eps)\n",
    "        boosting_tree.fit(X,y,y_pred)\n",
    "        y_pred += 0.1*boosting_tree.predict(X)\n",
    "        models.append(boosting_tree)\n",
    "    \n",
    "    return(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0297ef03-34b5-42cf-a4a0-93fb55651f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_predict(X,y,loss,eps = 0.1):\n",
    "    pred = np.zeros(X.shape[0])\n",
    "    for model in xgboost(X,y,loss=loss,eps = eps):\n",
    "        pred += 0.1*model.predict(X)\n",
    "    return np.full((X.shape[0], 1), 1.).flatten().astype('float64') + pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ff9e4f7d-e02f-43d6-9d80-5deadc376af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "K =20\n",
    "p =2\n",
    "N =300\n",
    "mu = np.random.normal(0.,4.,(K,2))\n",
    "component = np.random.randint(0,K,(N,))\n",
    "assignment = np.random.randint(0,2,(K,))\n",
    "X = mu[component,:] + np.random.normal(0.,1.,(N,p))\n",
    "y = assignment[component]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "73ae816d-da35-41d9-a0a3-0e55e05c6fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = xgboost(n_iters = 10,lr = 0.1,loss = sigmoid_loss)\n",
    "xgb.fit(X,y,eps=0.1)\n",
    "r =xgb.predict(X)\n",
    "predicted_probas = sigmoid(r)\n",
    "# manually set up the threshold for the binary classification\n",
    "# the xgboost.predict returns raw scores for each observation\n",
    "preds = np.where(predicted_probas > np.mean(predicted_probas), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8c0e2b2f-fdb5-4551-a814-3ec1d999b694",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(preds ==y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d183ffb6-8c2a-4a82-b0b0-c94ec1c7584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import e\n",
    "\n",
    "class Node:\n",
    "    \n",
    "    '''\n",
    "    A node object that is recursivly called within itslef to construct a regression tree. Based on Tianqi Chen's XGBoost \n",
    "    the internal gain used to find the optimal split value uses both the gradient and hessian. Also a weighted quantlie sketch \n",
    "    and optimal leaf values all follow Chen's description in \"XGBoost: A Scalable Tree Boosting System\" the only thing not \n",
    "    implemented in this version is sparsity aware fitting or the ability to handle NA values with a default direction.\n",
    "    Inputs\n",
    "    ------------------------------------------------------------------------------------------------------------------\n",
    "    x: pandas datframe of the training data\n",
    "    gradient: negative gradient of the loss function\n",
    "    hessian: second order derivative of the loss function\n",
    "    idxs: used to keep track of samples within the tree structure\n",
    "    subsample_cols: is an implementation of layerwise column subsample randomizing the structure of the trees\n",
    "    (complexity parameter)\n",
    "    min_leaf: minimum number of samples for a node to be considered a node (complexity parameter)\n",
    "    min_child_weight: sum of the heassian inside a node is a meaure of purity (complexity parameter)\n",
    "    depth: limits the number of layers in the tree\n",
    "    lambda: L2 regularization term on weights. Increasing this value will make model more conservative.\n",
    "    gamma: This parameter also prevents over fitting and is present in the the calculation of the gain (structure score). \n",
    "    As this is subtracted from the gain it essentially sets a minimum gain amount to make a split in a node.\n",
    "    eps: This parameter is used in the quantile weighted skecth or 'approx' tree method roughly translates to \n",
    "    (1 / sketch_eps) number of bins\n",
    "    Outputs\n",
    "    --------------------------------------------------------------------------------------------------------------------\n",
    "    A single tree object that will be used for gradient boosintg.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, x, gradient, hessian, idxs, subsample_cols = 0.8 , min_leaf = 5, min_child_weight = 1 ,depth = 10, lambda_ = 1, gamma = 1, eps = 0.1):\n",
    "      \n",
    "        self.x, self.gradient, self.hessian = x, gradient, hessian\n",
    "        self.idxs = idxs \n",
    "        self.depth = depth\n",
    "        self.min_leaf = min_leaf\n",
    "        self.lambda_ = lambda_\n",
    "        self.gamma  = gamma\n",
    "        self.min_child_weight = min_child_weight\n",
    "        self.row_count = len(idxs)\n",
    "        self.col_count = x.shape[1]\n",
    "        self.subsample_cols = subsample_cols\n",
    "        self.eps = eps\n",
    "        self.column_subsample = np.random.permutation(self.col_count)[:round(self.subsample_cols*self.col_count)]\n",
    "        \n",
    "        self.val = self.compute_gamma(self.gradient[self.idxs], self.hessian[self.idxs])\n",
    "          \n",
    "        self.score = float('-inf')\n",
    "        self.find_varsplit()\n",
    "        \n",
    "        \n",
    "    def compute_gamma(self, gradient, hessian):\n",
    "        '''\n",
    "        Calculates the optimal leaf value equation (5) in \"XGBoost: A Scalable Tree Boosting System\"\n",
    "        '''\n",
    "        return(-np.sum(gradient)/(np.sum(hessian) + self.lambda_))\n",
    "        \n",
    "    def find_varsplit(self):\n",
    "        '''\n",
    "        Scans through every column and calcuates the best split point.\n",
    "        The node is then split at this point and two new nodes are created.\n",
    "        Depth is only parameter to change as we have added a new layer to tre structure.\n",
    "        If no split is better than the score initalised at the begining then no splits further splits are made\n",
    "        '''\n",
    "        for c in self.column_subsample: self.find_greedy_split(c)\n",
    "        if self.is_leaf: return\n",
    "        x = self.split_col\n",
    "        lhs = np.nonzero(x <= self.split)[0]\n",
    "        rhs = np.nonzero(x > self.split)[0]\n",
    "        self.lhs = Node(x = self.x, gradient = self.gradient, hessian = self.hessian, idxs = self.idxs[lhs], min_leaf = self.min_leaf, depth = self.depth-1, lambda_ = self.lambda_ , gamma = self.gamma, min_child_weight = self.min_child_weight, eps = self.eps, subsample_cols = self.subsample_cols)\n",
    "        self.rhs = Node(x = self.x, gradient = self.gradient, hessian = self.hessian, idxs = self.idxs[rhs], min_leaf = self.min_leaf, depth = self.depth-1, lambda_ = self.lambda_ , gamma = self.gamma, min_child_weight = self.min_child_weight, eps = self.eps, subsample_cols = self.subsample_cols)\n",
    "        \n",
    "    def find_greedy_split(self, var_idx):\n",
    "        '''\n",
    "         For a given feature greedily calculates the gain at each split.\n",
    "         Globally updates the best score and split point if a better split point is found\n",
    "        '''\n",
    "        x = self.x[self.idxs, var_idx]\n",
    "        \n",
    "        for r in range(self.row_count):\n",
    "            lhs = x <= x[r]\n",
    "            rhs = x > x[r]\n",
    "            \n",
    "            lhs_indices = np.nonzero(x <= x[r])[0]\n",
    "            rhs_indices = np.nonzero(x > x[r])[0]\n",
    "            if(rhs.sum() < self.min_leaf or lhs.sum() < self.min_leaf \n",
    "               or self.hessian[lhs_indices].sum() < self.min_child_weight\n",
    "               or self.hessian[rhs_indices].sum() < self.min_child_weight): continue\n",
    "\n",
    "            curr_score = self.gain(lhs, rhs)\n",
    "            if curr_score > self.score: \n",
    "                self.var_idx = var_idx\n",
    "                self.score = curr_score\n",
    "                self.split = x[r]\n",
    "                \n",
    "    def weighted_qauntile_sketch(self, var_idx):\n",
    "        '''\n",
    "        XGBOOST Mini-Version\n",
    "        Yiyang \"Joe\" Zeng\n",
    "        Is an approximation to the eact greedy approach faster for bigger datasets wher it is not feasible\n",
    "        to calculate the gain at every split point. Uses equation (8) and (9) from \"XGBoost: A Scalable Tree Boosting System\"\n",
    "        '''\n",
    "        x = self.x[self.idxs, var_idx]\n",
    "        hessian_ = self.hessian[self.idxs]\n",
    "        df = pd.DataFrame({'feature':x,'hess':hessian_})\n",
    "        \n",
    "        df.sort_values(by=['feature'], ascending = True, inplace = True)\n",
    "        hess_sum = df['hess'].sum() \n",
    "        df['rank'] = df.apply(lambda x : (1/hess_sum)*sum(df[df['feature'] < x['feature']]['hess']), axis=1)\n",
    "        \n",
    "        for row in range(df.shape[0]-1):\n",
    "            # look at the current rank and the next ran\n",
    "            rk_sk_j, rk_sk_j_1 = df['rank'].iloc[row:row+2]\n",
    "            diff = abs(rk_sk_j - rk_sk_j_1)\n",
    "            if(diff >= self.eps):\n",
    "                continue\n",
    "                \n",
    "            split_value = (df['rank'].iloc[row+1] + df['rank'].iloc[row])/2\n",
    "            lhs = x <= split_value\n",
    "            rhs = x > split_value\n",
    "            \n",
    "            lhs_indices = np.nonzero(x <= split_value)[0]\n",
    "            rhs_indices = np.nonzero(x > split_value)[0]\n",
    "            if(rhs.sum() < self.min_leaf or lhs.sum() < self.min_leaf \n",
    "               or self.hessian[lhs_indices].sum() < self.min_child_weight\n",
    "               or self.hessian[rhs_indices].sum() < self.min_child_weight): continue\n",
    "                \n",
    "            curr_score = self.gain(lhs, rhs)\n",
    "            if curr_score > self.score: \n",
    "                self.var_idx = var_idx\n",
    "                self.score = curr_score\n",
    "                self.split = split_value\n",
    "                \n",
    "    def gain(self, lhs, rhs):\n",
    "        '''\n",
    "        Calculates the gain at a particular split point bases on equation (7) from\n",
    "        \"XGBoost: A Scalable Tree Boosting System\"\n",
    "        '''\n",
    "        gradient = self.gradient[self.idxs]\n",
    "        hessian  = self.hessian[self.idxs]\n",
    "        \n",
    "        lhs_gradient = gradient[lhs].sum()\n",
    "        lhs_hessian  = hessian[lhs].sum()\n",
    "        \n",
    "        rhs_gradient = gradient[rhs].sum()\n",
    "        rhs_hessian  = hessian[rhs].sum()\n",
    "        \n",
    "        gain = 0.5 *( (lhs_gradient**2/(lhs_hessian + self.lambda_)) + (rhs_gradient**2/(rhs_hessian + self.lambda_)) - ((lhs_gradient + rhs_gradient)**2/(lhs_hessian + rhs_hessian + self.lambda_))) - self.gamma\n",
    "        return(gain)\n",
    "                \n",
    "    @property\n",
    "    def split_col(self):\n",
    "        '''\n",
    "        splits a column \n",
    "        '''\n",
    "        return self.x[self.idxs , self.var_idx]\n",
    "                \n",
    "    @property\n",
    "    def is_leaf(self):\n",
    "        '''\n",
    "        checks if node is a leaf\n",
    "        '''\n",
    "        return self.score == float('-inf') or self.depth <= 0                 \n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.array([self.predict_row(xi) for xi in x])\n",
    "    \n",
    "    def predict_row(self, xi):\n",
    "        if self.is_leaf:\n",
    "            return(self.val)\n",
    "\n",
    "        node = self.lhs if xi[self.var_idx] <= self.split else self.rhs\n",
    "        return node.predict_row(xi)\n",
    "\n",
    "    \n",
    "class XGBoostTree:\n",
    "    '''\n",
    "    Wrapper class that provides a scikit learn interface to the recursive regression tree above\n",
    "    \n",
    "    Inputs\n",
    "    ------------------------------------------------------------------------------------------------------------------\n",
    "    x: pandas datframe of the training data\n",
    "    gradient: negative gradient of the loss function\n",
    "    hessian: second order derivative of the loss function\n",
    "    idxs: used to keep track of samples within the tree structure\n",
    "    subsample_cols: is an implementation of layerwise column subsample randomizing the structure of the trees\n",
    "    (complexity parameter)\n",
    "    min_leaf: minimum number of samples for a node to be considered a node (complexity parameter)\n",
    "    min_child_weight: sum of the heassian inside a node is a meaure of purity (complexity parameter)\n",
    "    depth: limits the number of layers in the tree\n",
    "    lambda: L2 regularization term on weights. Increasing this value will make model more conservative.\n",
    "    gamma: This parameter also prevents over fitting and is present in the the calculation of the gain (structure score). \n",
    "    As this is subtracted from the gain it essentially sets a minimum gain amount to make a split in a node.\n",
    "    eps: This parameter is used in the quantile weighted skecth or 'approx' tree method roughly translates to \n",
    "    (1 / sketch_eps) number of bins\n",
    "    \n",
    "    Outputs\n",
    "    --------------------------------------------------------------------------------------------------------------------\n",
    "    A single tree object that will be used for gradient boosintg.\n",
    "    \n",
    "    '''\n",
    "    def fit(self, x, gradient, hessian, subsample_cols = 0.8 , min_leaf = 5, min_child_weight = 1 ,depth = 10, lambda_ = 1, gamma = 1, eps = 0.1):\n",
    "        self.dtree = Node(x, gradient, hessian, np.array(np.arange(len(x))), subsample_cols, min_leaf, min_child_weight, depth, lambda_, gamma, eps)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.dtree.predict(X)\n",
    "   \n",
    "   \n",
    "class XGBoostClassifier:\n",
    "    '''\n",
    "    Full application of the XGBoost algorithm as described in \"XGBoost: A Scalable Tree Boosting System\" for \n",
    "    Binary Classification.\n",
    "    Inputs\n",
    "    ------------------------------------------------------------------------------------------------------------------\n",
    "    x: pandas datframe of the training data\n",
    "    gradient: negative gradient of the loss function\n",
    "    hessian: second order derivative of the loss function\n",
    "    idxs: used to keep track of samples within the tree structure\n",
    "    subsample_cols: is an implementation of layerwise column subsample randomizing the structure of the trees\n",
    "    (complexity parameter)\n",
    "    min_leaf: minimum number of samples for a node to be considered a node (complexity parameter)\n",
    "    min_child_weight: sum of the heassian inside a node is a meaure of purity (complexity parameter)\n",
    "    depth: limits the number of layers in the tree\n",
    "    lambda: L2 regularization term on weights. Increasing this value will make model more conservative.\n",
    "    gamma: This parameter also prevents over fitting and is present in the the calculation of the gain (structure score). \n",
    "    As this is subtracted from the gain it essentially sets a minimum gain amount to make a split in a node.\n",
    "    eps: This parameter is used in the quantile weighted skecth or 'approx' tree method roughly translates to \n",
    "    (1 / sketch_eps) number of bins\n",
    "    Outputs\n",
    "    --------------------------------------------------------------------------------------------------------------------\n",
    "    A single tree object that will be used for gradient boosintg.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.estimators = []\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    # first order gradient logLoss\n",
    "    def grad(self, preds, labels):\n",
    "        preds = self.sigmoid(preds)\n",
    "        return(preds - labels)\n",
    "    \n",
    "    # second order gradient logLoss\n",
    "    def hess(self, preds, labels):\n",
    "        preds = self.sigmoid(preds)\n",
    "        return(preds * (1 - preds))\n",
    "    \n",
    "    @staticmethod\n",
    "    def log_odds(column):\n",
    "        binary_yes = np.count_nonzero(column == 1)\n",
    "        binary_no  = np.count_nonzero(column == 0)\n",
    "        return(np.log(binary_yes/binary_no))\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, subsample_cols = 0.8 , min_child_weight = 1, depth = 5, min_leaf = 5, learning_rate = 0.4, boosting_rounds = 5, lambda_ = 1.5, gamma = 1, eps = 0.1):\n",
    "        self.X, self.y = X, y\n",
    "        self.depth = depth\n",
    "        self.subsample_cols = subsample_cols\n",
    "        self.eps = eps\n",
    "        self.min_child_weight = min_child_weight \n",
    "        self.min_leaf = min_leaf\n",
    "        self.learning_rate = learning_rate\n",
    "        self.boosting_rounds = boosting_rounds \n",
    "        self.lambda_ = lambda_\n",
    "        self.gamma  = gamma\n",
    "    \n",
    "        self.base_pred = np.full((X.shape[0], 1), 1).flatten().astype('float64')\n",
    "    \n",
    "        for booster in range(self.boosting_rounds):\n",
    "            Grad = self.grad(self.base_pred, self.y)\n",
    "            Hess = self.hess(self.base_pred, self.y)\n",
    "            boosting_tree = XGBoostTree().fit(self.X, Grad, Hess, depth = self.depth, min_leaf = self.min_leaf, lambda_ = self.lambda_, gamma = self.gamma, eps = self.eps, min_child_weight = self.min_child_weight, subsample_cols = self.subsample_cols)\n",
    "            self.base_pred += self.learning_rate * boosting_tree.predict(self.X)\n",
    "            self.estimators.append(boosting_tree)\n",
    "          \n",
    "    def predict_proba(self, X):\n",
    "        pred = np.zeros(X.shape[0])\n",
    "        \n",
    "        for estimator in self.estimators:\n",
    "            pred += self.learning_rate * estimator.predict(X) \n",
    "          \n",
    "        return(self.sigmoid(np.full((X.shape[0], 1), 1).flatten().astype('float64') + pred))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        pred = np.zeros(X.shape[0])\n",
    "        for estimator in self.estimators:\n",
    "            pred += self.learning_rate * estimator.predict(X) \n",
    "        \n",
    "        predicted_probas = self.sigmoid(np.full((X.shape[0], 1), 1).flatten().astype('float64') + pred)\n",
    "        preds = np.where(predicted_probas > np.mean(predicted_probas), 1, 0)\n",
    "        return(preds)\n",
    "       \n",
    "class XGBoostRegressor:\n",
    "    '''\n",
    "    Full application of the XGBoost algorithm as described in \"XGBoost: A Scalable Tree Boosting System\" for \n",
    "    regression.\n",
    "    Inputs\n",
    "    ------------------------------------------------------------------------------------------------------------------\n",
    "    x: pandas datframe of the training data\n",
    "    gradient: negative gradient of the loss function\n",
    "    hessian: second order derivative of the loss function\n",
    "    idxs: used to keep track of samples within the tree structure\n",
    "    subsample_cols: is an implementation of layerwise column subsample randomizing the structure of the trees\n",
    "    (complexity parameter)\n",
    "    min_leaf: minimum number of samples for a node to be considered a node (complexity parameter)\n",
    "    min_child_weight: sum of the heassian inside a node is a meaure of purity (complexity parameter)\n",
    "    depth: limits the number of layers in the tree\n",
    "    lambda: L2 regularization term on weights. Increasing this value will make model more conservative.\n",
    "    gamma: This parameter also prevents over fitting and is present in the the calculation of the gain (structure score). \n",
    "    As this is subtracted from the gain it essentially sets a minimum gain amount to make a split in a node.\n",
    "    eps: This parameter is used in the quantile weighted skecth or 'approx' tree method roughly translates to \n",
    "    (1 / sketch_eps) number of bins\n",
    "    Outputs\n",
    "    --------------------------------------------------------------------------------------------------------------------\n",
    "    A single tree object that will be used for gradient boosintg.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.estimators = []\n",
    "    \n",
    "    # first order gradient mean squared error\n",
    "    @staticmethod\n",
    "    def grad(preds, labels):\n",
    "        return(2*(preds-labels))\n",
    "    \n",
    "    # second order gradient logLoss\n",
    "    @staticmethod\n",
    "    def hess(preds, labels):\n",
    "        '''\n",
    "        hessian of mean squared error is a constant value of two \n",
    "        returns an array of twos\n",
    "        '''\n",
    "        return(np.full((preds.shape[0], 1), 2).flatten().astype('float64'))\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y, subsample_cols = 0.8 , min_child_weight = 1, depth = 5, min_leaf = 5, learning_rate = 0.4, boosting_rounds = 5, lambda_ = 1.5, gamma = 1, eps = 0.1):\n",
    "        self.X, self.y = X, y\n",
    "        self.depth = depth\n",
    "        self.subsample_cols = subsample_cols\n",
    "        self.eps = eps\n",
    "        self.min_child_weight = min_child_weight \n",
    "        self.min_leaf = min_leaf\n",
    "        self.learning_rate = learning_rate\n",
    "        self.boosting_rounds = boosting_rounds \n",
    "        self.lambda_ = lambda_\n",
    "        self.gamma  = gamma\n",
    "    \n",
    "        self.base_pred = np.full((X.shape[0], 1), np.mean(y)).flatten().astype('float64')\n",
    "    \n",
    "        for booster in range(self.boosting_rounds):\n",
    "            Grad = self.grad(self.base_pred, self.y)\n",
    "            Hess = self.hess(self.base_pred, self.y)\n",
    "            boosting_tree = XGBoostTree().fit(self.X, Grad, Hess, depth = self.depth, min_leaf = self.min_leaf, lambda_ = self.lambda_, gamma = self.gamma, eps = self.eps, min_child_weight = self.min_child_weight, subsample_cols = self.subsample_cols)\n",
    "            self.base_pred += self.learning_rate * boosting_tree.predict(self.X)\n",
    "            self.estimators.append(boosting_tree)\n",
    "          \n",
    "    def predict(self, X):\n",
    "        pred = np.zeros(X.shape[0])\n",
    "        \n",
    "        for estimator in self.estimators:\n",
    "            pred += self.learning_rate * estimator.predict(X) \n",
    "          \n",
    "        return np.full((X.shape[0], 1), np.mean(self.y)).flatten().astype('float64') + pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1c1fb234-6e6b-47f5-a99e-7993a438b51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = XGBoostClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2293f503-56e3-4a42-9d6f-db2bb5fc3f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.fit(X, y, subsample_cols = 1, min_child_weight = 1, depth = 5, min_leaf = 2, learning_rate = 0.1, boosting_rounds = 5, lambda_ = 1, gamma = 1, eps = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9bcd5935-d07b-443e-8452-d46d7282a324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8633333333333333"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a.predict(X) ==y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3013b5b1-5bad-4c9d-a827-e7563d8e9a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.580765290274654"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.predict_proba(X).mean()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4277bc40-2ac2-4518-9066-976772300c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
