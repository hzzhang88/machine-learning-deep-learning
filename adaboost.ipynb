{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b450b360-69dd-4d29-ab54-f072119caa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1083b394-499d-4982-9e7d-7d956b5ce3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeakTree(DecisionTreeClassifier):\n",
    "    def __init__(self,max_depth =1):\n",
    "        super().__init__(max_depth = max_depth)\n",
    "        self.polarity = 1\n",
    "        self.alpha = None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a9bedd79-16ba-4443-9024-f5a45e5a0fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaboost1():\n",
    "    def __init__(self, n_clf = 5):\n",
    "        self.n_clf = n_clf\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        n_samples, n_features = np.shape(X)\n",
    "        w = np.full(n_samples,1/(n_samples))\n",
    "        self.clfs = []\n",
    "        for _ in range(self.n_clf):\n",
    "            clf = WeakTree(max_depth = 1)\n",
    "            clf.fit(X,y,sample_weight = w)\n",
    "            error =1- clf.score(X,y,w)\n",
    "            clf.alpha = 0.5*math.log((1.-error)/(error+1e-10))\n",
    "            pred_y = clf.predict(X)\n",
    "            cc = (pred_y == y)*1.0\n",
    "            cc[cc==0] = -1.0\n",
    "            w = w*np.exp(-cc*clf.alpha)\n",
    "            w /=np.sum(w)\n",
    "            self.clfs.append(clf)\n",
    "        \n",
    "    def predict(self,X):\n",
    "        y_pred = np.zeros((X.shape[0],))\n",
    "        for tree in self.clfs:\n",
    "            y_hat = tree.predict(X)\n",
    "            y_hat[y_hat == 0] =-1.\n",
    "            y_pred += tree.alpha*y_hat\n",
    "        \n",
    "        return np.sign(y_pred)\n",
    "    \n",
    "    def accuracy(self,X,y):\n",
    "        y_ = y.copy()\n",
    "        y_[y_ ==0] = -1.\n",
    "        \n",
    "        y_pred= self.predict(X)\n",
    "        return (y_ == y_pred).mean()\n",
    "                    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "618fb780-6a13-477c-81c1-ee1b8cb79444",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "K =20\n",
    "p =2\n",
    "N =300\n",
    "mu = np.random.normal(0.,4.,(K,2))\n",
    "component = np.random.randint(0,K,(N,))\n",
    "assignment = np.random.randint(0,2,(K,))\n",
    "X = mu[component,:] + np.random.normal(0.,1.,(N,p))\n",
    "y = assignment[component]\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d5ae6ff0-d44c-469a-802e-fe162732566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "        n_samples, n_features = np.shape(X)\n",
    "        w = np.full(n_samples,1/(n_samples))\n",
    "        clfs = []\n",
    "        for _ in range(10):\n",
    "            clf = WeakTree(max_depth = 1)\n",
    "            clf.fit(X,y,sample_weight = w)\n",
    "            error =1- clf.score(X,y,w)\n",
    "            clf.alpha = 0.5*math.log((1.-error)/(error+1e-10))\n",
    "            pred_y = clf.predict(X)\n",
    "            cc = (pred_y == y)*1.0\n",
    "            cc[cc==0] = -1.0\n",
    "            w = w*np.exp(-cc*clf.alpha)\n",
    "            w /=np.sum(w)\n",
    "            clfs.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9c9566a1-486f-4f0e-a0f0-88d64ad07321",
   "metadata": {},
   "outputs": [],
   "source": [
    "        y_pred = np.zeros((X.shape[0],))\n",
    "        for tree in clfs:\n",
    "            y_hat = tree.predict(X)\n",
    "            y_hat[y_hat ==0] = -1.\n",
    "            y_pred += tree.alpha*y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7d1b6616-034c-4c36-9077-8c32bbc75471",
   "metadata": {},
   "outputs": [],
   "source": [
    "sign = np.sign(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8909d0d8-e9a3-49bb-9fb3-5016cc06151e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[y == 0] = -1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0ccf1bb5-5f30-4fe4-8fae-19b771d717cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y ==sign).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3342c222-9ba2-46c3-ba4c-4c45254e9cd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad1 = Adaboost1(20)\n",
    "ad1.fit(X,y)\n",
    "ad1.accuracy(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "73c867a1-440f-42f8-8948-dc8a892c647a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8533333333333334"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad = Adaboost(20)\n",
    "ad.fit(X,y_)\n",
    "(ad.predict(X) ==y_).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a8a44905-c7a9-4b82-be45-6846e75ff96d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(n_estimators=20)\n",
    "clf.fit(X, y)\n",
    "(clf.predict(X) ==y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f05f5590-0aa7-4593-adb6-244b83cbdb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionStump():\n",
    "    def __init__(self):\n",
    "        # Determines if sample shall be classified as -1 or 1 given threshold\n",
    "        self.polarity = 1\n",
    "        # The index of the feature used to make classification\n",
    "        self.feature_index = None\n",
    "        # The threshold value that the feature should be measured against\n",
    "        self.threshold = None\n",
    "        # Value indicative of the classifier's accuracy\n",
    "        self.alpha = None\n",
    "\n",
    "class Adaboost():\n",
    "    \"\"\"Boosting method that uses a number of weak classifiers in \n",
    "    ensemble to make a strong classifier. This implementation uses decision\n",
    "    stumps, which is a one level Decision Tree. \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_clf: int\n",
    "        The number of weak classifiers that will be used. \n",
    "    \"\"\"\n",
    "    def __init__(self, n_clf=5):\n",
    "        self.n_clf = n_clf\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = np.shape(X)\n",
    "\n",
    "        # Initialize weights to 1/N\n",
    "        w = np.full(n_samples, (1 / n_samples))\n",
    "        \n",
    "        self.clfs = []\n",
    "        # Iterate through classifiers\n",
    "        for _ in range(self.n_clf):\n",
    "            clf = DecisionStump()\n",
    "            # Minimum error given for using a certain feature value threshold\n",
    "            # for predicting sample label\n",
    "            min_error = float('inf')\n",
    "            # Iterate throught every unique feature value and see what value\n",
    "            # makes the best threshold for predicting y\n",
    "            for feature_i in range(n_features):\n",
    "                feature_values = np.expand_dims(X[:, feature_i], axis=1)\n",
    "                unique_values = np.unique(feature_values)\n",
    "                # Try every unique feature value as threshold\n",
    "                for threshold in unique_values:\n",
    "                    p = 1\n",
    "                    # Set all predictions to '1' initially\n",
    "                    prediction = np.ones(np.shape(y))\n",
    "                    # Label the samples whose values are below threshold as '-1'\n",
    "                    prediction[X[:, feature_i] < threshold] = -1\n",
    "                    # Error = sum of weights of misclassified samples\n",
    "                    error = sum(w[y != prediction])\n",
    "                    \n",
    "                    # If the error is over 50% we flip the polarity so that samples that\n",
    "                    # were classified as 0 are classified as 1, and vice versa\n",
    "                    # E.g error = 0.8 => (1 - error) = 0.2\n",
    "                    if error > 0.5:\n",
    "                        error = 1 - error\n",
    "                        p = -1\n",
    "\n",
    "                    # If this threshold resulted in the smallest error we save the\n",
    "                    # configuration\n",
    "                    if error < min_error:\n",
    "                        clf.polarity = p\n",
    "                        clf.threshold = threshold\n",
    "                        clf.feature_index = feature_i\n",
    "                        min_error = error\n",
    "            # Calculate the alpha which is used to update the sample weights,\n",
    "            # Alpha is also an approximation of this classifier's proficiency\n",
    "            clf.alpha = 0.5 * math.log((1.0 - min_error) / (min_error + 1e-10))\n",
    "            # Set all predictions to '1' initially\n",
    "            predictions = np.ones(np.shape(y))\n",
    "            # The indexes where the sample values are below threshold\n",
    "            negative_idx = (clf.polarity * X[:, clf.feature_index] < clf.polarity * clf.threshold)\n",
    "            # Label those as '-1'\n",
    "            predictions[negative_idx] = -1\n",
    "            # Calculate new weights \n",
    "            # Missclassified samples gets larger weights and correctly classified samples smaller\n",
    "            w *= np.exp(-clf.alpha * y * predictions)\n",
    "            # Normalize to one\n",
    "            w /= np.sum(w)\n",
    "\n",
    "            # Save classifier\n",
    "            self.clfs.append(clf)\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_samples = np.shape(X)[0]\n",
    "        y_pred = np.zeros((n_samples, 1))\n",
    "        # For each classifier => label the samples\n",
    "        for clf in self.clfs:\n",
    "            # Set all predictions to '1' initially\n",
    "            predictions = np.ones(np.shape(y_pred))\n",
    "            # The indexes where the sample values are below threshold\n",
    "            negative_idx = (clf.polarity * X[:, clf.feature_index] < clf.polarity * clf.threshold)\n",
    "            # Label those as '-1'\n",
    "            predictions[negative_idx] = -1\n",
    "            # Add predictions weighted by the classifiers alpha\n",
    "            # (alpha indicative of classifier's proficiency)\n",
    "            y_pred += clf.alpha * predictions\n",
    "\n",
    "        # Return sign of prediction sum\n",
    "        y_pred = np.sign(y_pred).flatten()\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9804ab5b-a9c3-44f8-824b-9006cea472e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
